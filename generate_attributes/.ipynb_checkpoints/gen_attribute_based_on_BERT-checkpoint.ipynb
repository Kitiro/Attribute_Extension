{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzc/anaconda3/envs/exp/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-88a000441f53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pretrained_model/wiki-news-300d-1M.vec'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/exp/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1630\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1631\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1632\u001b[0;31m             \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1633\u001b[0m         )\n\u001b[1;32m   1634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/exp/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   1911\u001b[0m             )\n\u001b[1;32m   1912\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1913\u001b[0;31m             \u001b[0m_word2vec_read_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1914\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1915\u001b[0m         logger.info(\n",
      "\u001b[0;32m~/anaconda3/envs/exp/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_word2vec_read_text\u001b[0;34m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)\u001b[0m\n\u001b[1;32m   1817\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unexpected end of input; is count incorrect or file otherwise damaged?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1818\u001b[0m         \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_word2vec_line_to_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1819\u001b[0;31m         \u001b[0m_add_word_to_kv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/exp/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_add_word_to_kv\u001b[0;34m(kv, counts, word, weights, vocab_size)\u001b[0m\n\u001b[1;32m   1768\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vocabulary file is incomplete: '%s' is missing\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1769\u001b[0m         \u001b[0mword_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1770\u001b[0;31m     \u001b[0mkv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_vecattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/exp/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mset_vecattr\u001b[0;34m(self, key, attr, val)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \"\"\"\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallocate_vecattrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpandos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/exp/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mallocate_vecattrs\u001b[0;34m(self, attrs, types)\u001b[0m\n\u001b[1;32m    297\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0mprev_expando\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpandos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_expando\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 raise TypeError(\n\u001b[1;32m    301\u001b[0m                     \u001b[0;34mf\"Can't allocate type {t} for attribute {attr}, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/exp/lib/python3.7/site-packages/numpy/core/numerictypes.py\u001b[0m in \u001b[0;36missubdtype\u001b[0;34m(arg1, arg2)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \"\"\"\n\u001b[0;32m--> 392\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0missubclass_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m         \u001b[0marg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0missubclass_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "import os \n",
    "from datetime import datetime\n",
    "import random \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format('pretrained_model/wiki-news-300d-1M.vec', binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzc/anaconda3/envs/exp/lib/python3.7/site-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '_pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-eb9c89ae6b74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# 序列化\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pretrained_model/pickled_model_300d'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0m_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name '_pickle' is not defined"
     ]
    }
   ],
   "source": [
    "# load glove \n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    " \n",
    "# 输入文件\n",
    "glove_file = 'pretrained_model/glove.6B.300d.txt'\n",
    " \n",
    "# 输出文件\n",
    "tmp_file = \"pretrained_model/glove_word2vec_300d.txt\"\n",
    " \n",
    "# 转换成word2vec的词向量\n",
    "_ = glove2word2vec(glove_file, tmp_file)\n",
    " \n",
    "# 加载转化后的文件\n",
    "model = KeyedVectors.load_word2vec_format(tmp_file, binary=False)\n",
    " \n",
    "import _pickle \n",
    "# 序列化\n",
    "with open('pretrained_model/pickled_model_300d', 'wb') as f:\n",
    "    _pickle.dump(model, f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.761861\n"
     ]
    }
   ],
   "source": [
    "# 测试模型\n",
    "f = open('pretrained_model/pickled_model_300d', 'rb')\n",
    "model = _pickle.load(f)\n",
    "print(model.similarity('have','has'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_serving.client import BertClient\n",
    "bc = BertClient()\n",
    "a = bc.encode(['panda'])  # a[0][0]: [cls] a[0][1]: [word] a[0][2]: [sep] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['antelope', 'grizzly+bear', 'killer+whale', 'beaver', 'dalmatian', 'persian+cat', 'horse', 'german+shepherd', 'blue+whale', 'siamese+cat', 'skunk', 'mole', 'tiger', 'hippopotamus', 'leopard', 'moose', 'spider+monkey', 'humpback+whale', 'elephant', 'gorilla', 'ox', 'fox', 'sheep', 'seal', 'chimpanzee', 'hamster', 'squirrel', 'rhinoceros', 'rabbit', 'bat', 'giraffe', 'wolf', 'chihuahua', 'rat', 'weasel', 'otter', 'buffalo', 'zebra', 'giant+panda', 'deer', 'bobcat', 'pig', 'lion', 'mouse', 'polar+bear', 'collie', 'walrus', 'raccoon', 'cow', 'dolphin']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'bc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-be48ba16a2cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mawa2_class\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mvec_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#直接取首cls的embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bc' is not defined"
     ]
    }
   ],
   "source": [
    "# 为AWA2的类产生vector\n",
    "awa2_class = []\n",
    "#读取动物列表\n",
    "with open('data/awa_class.txt') as f:\n",
    "    lines=f.readlines()\n",
    "    for i in lines:\n",
    "        awa2_class.append(i.strip())\n",
    "\n",
    "vec_dict = {}\n",
    "print(awa2_class)\n",
    "\n",
    "for i in awa2_class:\n",
    "    vec_dict[i] = bc.encode([i])[0][0] #直接取首cls的embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cub_class = []\n",
    "with open('/home/zzc/downloads/CUB_200/CUB_200_2011/classes.txt') as f:\n",
    "    lines=f.readlines()\n",
    "    for line in lines:\n",
    "        name = line.split('.')[1].strip()\n",
    "        cub_class.append(name)\n",
    "print(cub_class)\n",
    "\n",
    "vec_dict = {}\n",
    "\n",
    "for i in cub_class:\n",
    "    vec_dict[i] = bc.encode([i])[0][0] #直接取首cls的embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为SUN数据集的类别产生特征文件\n",
    "from scipy import io as sio\n",
    "import numpy as np\n",
    "dataset = 'SUN_data'\n",
    "image_embedding = 'res101'\n",
    "class_embedding = 'att' # original_att\n",
    "\n",
    "path = '/home/zzc/exp/zsl/DeepEmbeddingModel_ZSL-pytorch/SUN_data/'\n",
    "\n",
    "matcontent = sio.loadmat(path+'res101.mat')\n",
    "\n",
    "label = matcontent['labels'].astype(int).squeeze() - 1\n",
    "\n",
    "matcontent = sio.loadmat(path+'att_splits.mat')\n",
    "trainval_loc = matcontent['trainval_loc'].squeeze() - 1\n",
    "train_label = np.unique(label[trainval_loc].astype(int))  # 645 \n",
    "\n",
    "test_unseen_loc = matcontent['test_unseen_loc'].squeeze() - 1\n",
    "test_label = np.unique(label[test_unseen_loc].astype(int)) # 72 class\n",
    "\n",
    "with open(path +'trainvalclasses.txt') as f:\n",
    "    lines=f.readlines()\n",
    "    train_class = [i.strip() for i in lines]\n",
    "with open(path +'testclasses.txt') as f:\n",
    "    lines=f.readlines()\n",
    "    test_class = [i.strip() for i in lines]\n",
    "sun_class = []\n",
    "train_index = 0\n",
    "test_index = 0\n",
    "for i in range(717):\n",
    "    if i in train_label:\n",
    "        sun_class.append(train_class[train_index])\n",
    "        train_index += 1\n",
    "    else:\n",
    "        sun_class.append(test_class[test_index])\n",
    "        test_index += 1\n",
    "print(sun_class[:100])\n",
    "vec_dict = {}\n",
    "\n",
    "for i in sun_class:\n",
    "    vec_dict[i] = bc.encode([i])[0][0] #直接取首cls的embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为APY数据集的类别产生特征文件\n",
    "from scipy import io as sio\n",
    "import numpy as np\n",
    "dataset = 'APY_data/'\n",
    "\n",
    "path = '/home/zzc/exp/zsl/DeepEmbeddingModel_ZSL-pytorch/'+dataset\n",
    "\n",
    "matcontent = sio.loadmat(path+'res101.mat')\n",
    "\n",
    "label = matcontent['labels'].astype(int).squeeze() - 1\n",
    "\n",
    "matcontent = sio.loadmat(path+'att_splits.mat')\n",
    "trainval_loc = matcontent['trainval_loc'].squeeze() - 1\n",
    "train_label = np.unique(label[trainval_loc].astype(int))  # 645 \n",
    "\n",
    "print(train_label)\n",
    "test_unseen_loc = matcontent['test_unseen_loc'].squeeze() - 1\n",
    "test_label = np.unique(label[test_unseen_loc].astype(int)) # 72 class\n",
    "print(test_label)\n",
    "\n",
    "with open(path +'trainvalclasses.txt') as f:\n",
    "    lines=f.readlines()\n",
    "    train_class = [i.strip() for i in lines]\n",
    "with open(path +'testclasses.txt') as f:\n",
    "    lines=f.readlines()\n",
    "    test_class = [i.strip() for i in lines]\n",
    "apy_class = []\n",
    "print(apy_class[:100])\n",
    "train_index = 0\n",
    "test_index = 0\n",
    "for i in range(32):\n",
    "    if i in train_label:\n",
    "        apy_class.append(train_class[train_index])\n",
    "        train_index += 1\n",
    "    else:\n",
    "        apy_class.append(test_class[test_index])\n",
    "        test_index += 1\n",
    "\n",
    "vec_dict = {}\n",
    "for i in apy_class:\n",
    "    vec_dict[i] = bc.encode([i])[0][0] #直接取首cls的embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有w2v的API产生最相似的词\n",
    "\n",
    "# 需要指定此处的class_list\n",
    "class_list = apy_class\n",
    "\n",
    "all = []\n",
    "lost = []\n",
    "top_k = 2000\n",
    "dict_name = {}\n",
    "\n",
    "adj = ['JJ','JJR','JJS']\n",
    "\n",
    "#把所有生成属性存成一维数组\n",
    "for i in class_list:\n",
    "#     print(i)\n",
    "#     items = i.lower().split('_')\n",
    "#     new_name = \"\"\n",
    "#     for item in items:\n",
    "#         if nltk.pos_tag([item])[0][1] not in adj: #过滤掉类别名中的形容词，针对CUB data\n",
    "#             new_name += item+'-'\n",
    "#     i = new_name.strip('-')\n",
    "#     print(i)\n",
    "#     print('*'*10)\n",
    "    try:\n",
    "        class_attr = model.most_similar(i.replace('+','-'), topn=top_k)\n",
    "        class_attr = [ j[0] for j in class_attr]  #只取name 舍弃相似度\n",
    "        dict_name[i] = class_attr\n",
    "        all.extend(class_attr)\n",
    "    except:\n",
    "        lost.append(i)\n",
    "        \n",
    "#把得到的所有属性中不属于adj的筛选掉\n",
    "result_adj = [nltk.pos_tag([word])[0][0] for word in all if nltk.pos_tag([word])[0][1] in adj]\n",
    "print(result_adj[:50])\n",
    "print('word num(after filtering non-adj word)',len(result_adj))\n",
    "\n",
    "classes = [j for j in class_list if j not in lost]\n",
    "print('total class num:',len(class_list))\n",
    "print(len(lost),'class are threw')\n",
    "print(lost[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#按词频排序\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "num_wanted = 300 #想要的属性数量, awa2-50, cub-312, sun-102, apy-64\n",
    "\n",
    "counter = Counter(result_adj)\n",
    "attr_selected = counter.most_common(num_wanted)\n",
    "\n",
    "# 通过cosine相似度为类与生成的属性进行赋值\n",
    "attr_vec_dict = {}\n",
    "for attr in attr_selected:\n",
    "    attr_vec = bc.encode([attr[0]])[0][0]\n",
    "    attr_vec_dict[attr] = attr_vec\n",
    "attribute_matrix = []\n",
    "for i in class_list: \n",
    "    class_vec = vec_dict[i]\n",
    "    attr_sim = []\n",
    "    for attr in attr_selected:\n",
    "        attr_vec = attr_vec_dict[attr]\n",
    "        sim = cosine_similarity([class_vec,attr_vec])\n",
    "        attr_sim.append(sim[0][1])\n",
    "\n",
    "    attribute_matrix.append(attr_sim)\n",
    "attribute_matrix = np.array(attribute_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "attribute_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attribute_matrix = np.array(attribute_matrix)\n",
    "#np.save('/home/zzc/exp/zsl/generate_attributes/generated_attributes/class_attribute_map_cub_'+str(num_wanted)+'.npy',attribute_matrix)\n",
    "np.save('/home/zzc/exp/zsl/generate_attributes/generated_attributes/class_attribute_map_apy.npy',attribute_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imagenet结点名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_serving.client import BertClient\n",
    "bc = BertClient()\n",
    "a = bc.encode(['panda'])  # a[0][0]: [cls] a[0][1]: [word] a[0][2]: [sep] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-8bc71255a22e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import json\n",
    "def getnode(x):\n",
    "    return wn.synset_from_pos_and_offset('n', int(x[1:]))\n",
    "\n",
    "def getwnid(u):\n",
    "    s = str(u.offset())\n",
    "    return 'n' + (8 - len(s)) * '0' + s\n",
    "\n",
    "from os.path import join as osp\n",
    "\n",
    "save_dir = '/home/zzc/exp/zsl_code/19CVPR_GCN_imagenet_DGP/materials'\n",
    "\n",
    "js = json.load(open(osp(save_dir,'imagenet-induced-graph.json'), 'r'))\n",
    "wnids = js['wnids']\n",
    "name_list = [getnode(wnid).lemma_names()[0] for wnid in wnids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32324it [49:01, 10.99it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "dimension = 768\n",
    "embedding = dict()\n",
    "for wnid, name in tqdm(zip(wnids, name_list)):\n",
    "    terms = name.lower().replace('_', ' ').replace('-', ' ').split(' ')\n",
    "    res = bc.encode(terms)\n",
    "    embedding[wnid] = np.mean(res, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32295"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# js = json.load(open(osp(save_dir,'imagenet-induced-graph.json'), 'r'))\n",
    "# wnids = js['wnids']\n",
    "vectors = [embedding[w].tolist() for w in wnids]\n",
    "\n",
    "obj = {}\n",
    "obj['wnids'] = wnids\n",
    "obj['vectors'] = vectors\n",
    "json.dump(obj, open(osp(save_dir, 'bert_large_embedding.json'), 'w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
